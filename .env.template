# IP 白名單（多段以逗號分隔，預設僅允許 192.168.0.0/16）
OLLAMA_IP_WHITELIST=192.168.0.0/16
# Traefik Variables
TRAEFIK_IMAGE_TAG=traefik:2.9
TRAEFIK_LOG_LEVEL=WARN
TRAEFIK_ACME_EMAIL=
TRAEFIK_HOSTNAME=
# Basic Authentication for Traefik Dashboard
# Username: traefikadmin
# Passwords must be encoded using MD5, SHA1, or BCrypt https://hostingcanada.org/htpasswd-generator/
TRAEFIK_BASIC_AUTH=

# Ollama Variables
# For configurations using AMD GPUs, use ollama/ollama:rocm
OLLAMA_IMAGE_TAG=ollama/ollama
OLLAMA_HOSTNAME=
# Define the models to be installed in the Ollama service. Separate each model name with a comma
# Recommended sources for finding models compatible with Ollama:
# 1. Ollama Official Website - https://ollama.com/library: Visit the official Ollama website for detailed information 
#    on available models and how to use them with the platform
# 2. Hugging Face Model Hub - https://huggingface.co/models: Ollama supports many models from Hugging Face
#    You can search and explore models across various categories such as NLP, vision, and more
# 3. Open WebUI - https://openwebui.com: While primarily designed for Open WebUI, it hosts a range of models 
#    that might also be adaptable for use with Ollama. Ensure compatibility before use
OLLAMA_INSTALL_MODELS=llama3.1
# Number of GPUs to allocate to the Ollama service.
# Specify '1' to use a single GPU, ideal for environments where resources are shared or for less GPU-intensive tasks
# Use 'all' to allocate all available GPUs, which is suitable for high-performance tasks that benefit from parallel computing across multiple GPUs
# Make sure to uncomment and configure the corresponding GPU section in the YAML file for NVIDIA GPU support if you wish to use your GPU for processing
OLLAMA_GPU_COUNT=all

# Open WebUI Variables
WEBUI_IMAGE_TAG=ghcr.io/open-webui/open-webui:0.3
